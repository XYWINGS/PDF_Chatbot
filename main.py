# -*- coding: utf-8 -*-
"""CTSE_2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ETCfoDrp-dTpK7jLio0eabfYsqadxUnH
"""

# Commented out IPython magic to ensure Python compatibility.
# %pip install --quiet --upgrade langchain-text-splitters langchain-community langgraph

# Commented out IPython magic to ensure Python compatibility.
# %pip install langchain openai faiss-cpu pypdf tiktoken sentence-transformers -q

# Commented out IPython magic to ensure Python compatibility.
# %pip install -qU langchain-core

# Commented out IPython magic to ensure Python compatibility.
# %pip install huggingface_hub[hf_xet]

# Commented out IPython magic to ensure Python compatibility.
# %pip install -U langchain-huggingface

# Install required packages

# Import libraries
from langchain.document_loaders import PyPDFLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.vectorstores import FAISS
from langchain.llms import HuggingFaceHub
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationalRetrievalChain
from langchain_huggingface import HuggingFaceEndpoint
from dotenv import load_dotenv

import os

from google.colab import files
uploaded = files.upload()  # Upload PDF's

# Get list of uploaded PDFs
pdf_files = [f for f in os.listdir() if f.endswith('.pdf')]
print("PDF FILES",pdf_files)
# Load documents
documents = []
for pdf_file in pdf_files:
    loader = PyPDFLoader(pdf_file)
    documents.extend(loader.load())

# Split documents
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200
)
texts = text_splitter.split_documents(documents)

# Using sentence transformers embeddings
embeddings = HuggingFaceEmbeddings(model_name="all-MiniLM-L6-v2")

# Create vector store
vectorstore = FAISS.from_documents(texts, embeddings)


load_dotenv()  # Load environment variables from .env
api_token = os.getenv("HUGGINGFACEHUB_API_TOKEN")

if not api_token:
    raise ValueError("Hugging Face API token not set in environment!")

os.environ["HUGGINGFACEHUB_API_TOKEN"] = api_token


# Using HuggingFace

llm = HuggingFaceHub(
    repo_id="HuggingFaceH4/zephyr-7b-beta",
    task="text-generation",
    model_kwargs={
        "temperature": 0.1,
        "max_new_tokens": 512,
        "do_sample": True
    }
)

# Conversation memory
memory = ConversationBufferMemory(
    memory_key="chat_history",
    return_messages=False
)

# Create QA chain
qa_chain = ConversationalRetrievalChain.from_llm(
    llm=llm,
    retriever=vectorstore.as_retriever(),
    memory=memory,
    chain_type="stuff"
)

def chat_with_notes(question):
    response = qa_chain.invoke({"question": question})
    return response["answer"]

# Test it
print(chat_with_notes("What are the Benefits of Microservices Architecture?"))